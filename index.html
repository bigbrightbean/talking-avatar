<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Talking Avatar</title>
<style>
body {
  font-family: 'Comic Sans MS', Arial, sans-serif;
  text-align: center;
  background: #f0f8ff;
  padding: 40px;
}
h1 { color: #ff6600; margin-bottom: 30px; }
#avatar { width: 200px; margin-bottom: 20px; }
#text { font-size: 18px; margin-top: 20px; min-height: 60px; }
#talkBtn {
  font-size: 20px; padding: 20px 30px; border: none; border-radius: 50px;
  cursor: pointer; color: white; background: #4CAF50; box-shadow: 0 8px #666;
  transition: all 0.15s;
}
#talkBtn:active { box-shadow: 0 4px #444; transform: translateY(4px); }
#talkBtn.orange { background: #FF9933; }
</style>
</head>
<body>

<h1>Talk to Me!</h1>
<img id="avatar" src="https://i.imgur.com/1X6R7Yx.png" alt="Avatar">
<button id="talkBtn">ðŸŽ¤ Talk</button>
<div id="text"></div>

<script>
const workerURL = "https://talking-avatar.bigbrightbean.workers.dev/"; // your Worker URL
const talkBtn = document.getElementById("talkBtn");
const textDiv = document.getElementById("text");

let recognizing = false;
let transcripts = []; // store batch transcripts
let recognition;

// cross-browser
const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

// --- start a new batch ---
function startRecognitionBatch() {
  recognition = new SpeechRecognition();
  recognition.lang = "en-US";
  recognition.continuous = true;
  recognition.interimResults = false;

  recognition.onresult = event => {
    let transcript = "";
    for (let i = 0; i < event.results.length; i++) {
      transcript += event.results[i][0].transcript;
    }
    transcripts.push(transcript);
    textDiv.innerText = "You said: " + transcripts.join(" ");
  };

  recognition.onerror = event => {
    console.error(event);
    alert("Microphone error. Try again.");
    stopRecognition();
  };

  recognition.onend = () => {
    if (recognizing) {
      // restart automatically for next 60s batch
      recognition.start();
    }
  };

  recognition.start();
}

// --- stop recording ---
function stopRecognition() {
  if (recognition) {
    recognition.stop();
    recognition = null;
  }
}

// --- send combined text to AI ---
async function sendToAI() {
  const finalText = transcripts.join(" ").trim();
  if (!finalText) {
    textDiv.innerText = "No speech detected.";
    return;
  }

  textDiv.innerText = "Thinking...";

  try {
    const resp = await fetch(workerURL, {
      method: "POST",
      headers: {"Content-Type":"application/json"},
      body: JSON.stringify({ text: finalText })
    });

    const data = await resp.json();
    textDiv.innerText = "Avatar says: " + data.reply;

    const utter = new SpeechSynthesisUtterance(data.reply);
    utter.rate = 0.9;
    utter.pitch = 1.1;
    speechSynthesis.speak(utter);

  } catch(err) {
    console.error(err);
    textDiv.innerText = "Error contacting AI.";
  }
}

// --- Talk button toggle ---
talkBtn.addEventListener("click", () => {
  if (!recognizing) {
    // start recording
    recognizing = true;
    transcripts = []; // clear previous
    talkBtn.classList.add("orange");
    talkBtn.innerText = "ðŸŽ¤ Listening...";
    startRecognitionBatch();
  } else {
    // stop recording
    recognizing = false;
    talkBtn.classList.remove("orange");
    talkBtn.innerText = "ðŸŽ¤ Talk";
    stopRecognition();
    sendToAI(); // send final text to AI
  }
});
</script>

</body>
</html>
